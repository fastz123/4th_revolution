{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 대소문자 통일\n",
    "2. 숫자, 문장부호, 특수문자 제거\n",
    "3. 불용어 제거\n",
    "4. 어근 통일화\n",
    "5. N_gram\n",
    "6. 품사분석\n",
    "7. Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 대소문자 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hello World'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 숫자, 문장부호, 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'서울 부동산 가격이 올해 들어 평균 % 상승했습니다'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sub('','서울 부동산 가격이 올해 들어 평균 30% 상승했습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('\\W+') # 문장부호 및 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제_1 서울 부동산 가격이 올해 들어 평균 30 상승했습니다 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sub(' ','주제_1 : *서울 부동산 가격이 올해 들어 평균 30% 상승했습니다!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주제 1 : *서울 부동산 가격이 올해 들어 평균 30% 상승했습니다!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.sub(' ','주제_1 : *서울 부동산 가격이 올해 들어 평균 30% 상승했습니다!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_korean = ['추적','연휴','민족','대이동','시작','늘어','교통량','교통사고','특히','자동차',\n",
    "                '고창','상당수','차지','나타','것','기자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['가다','늘어','나타','것','기자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추적', '연휴', '민족', '대이동', '시작', '교통량', '교통사고', '특히', '자동차', '고창', '상당수', '차지']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in words_korean if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_english = ['chief','justice','roberts','president','carter','clinton','bush','president','obama',\n",
    "                'fellow',',','american','and','people','of',',','the','world','thank','you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chief',\n",
       " 'justice',\n",
       " 'roberts',\n",
       " 'president',\n",
       " 'carter',\n",
       " 'clinton',\n",
       " 'bush',\n",
       " 'president',\n",
       " 'obama',\n",
       " 'fellow',\n",
       " ',',\n",
       " 'american',\n",
       " 'people',\n",
       " ',',\n",
       " 'world',\n",
       " 'thank']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in words_english if not w in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 같은 어근 동일화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = 'It is important to be immersed while you are pythoning with python. All pythoners have\\\n",
    " phythoned poorly at least once.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'is', 'important', 'to', 'be', 'immersed', 'while', 'you', 'are', 'pythoning', 'with', 'python', '.', 'All', 'pythoners', 'have', 'phythoned', 'poorly', 'at', 'least', 'once', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(new_text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is import to be immers while you are python with python . all python have phython poorli at least onc . "
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(ps_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS_stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is import to be immers whil you ar python with python . al python hav phython poor at least ont . "
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(LS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.regexp import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_stemmer = RegexpStemmer('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is important to be immersed while you are pythoning with python . All pythoners have phythoned poorly at least once . "
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    print(RS_stemmer.stem(w), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama,\\\n",
    " fellow Americans and people of the world, thank you. We, the citizens of America are now joined in a\\\n",
    "  great national effort to rebuild our country and restore its promise for all of our people. Together\\\n",
    "  , we will determine the course of America and the world for many, many years to come. we will \\\n",
    "  challenges. We will confront hardships, but we will get the job done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chief', 'Justice') ('Justice', 'Roberts,') ('Roberts,', 'President') ('President', 'Carter,') ('Carter,', 'President') ('President', 'Clinton,') ('Clinton,', 'President') ('President', 'Bush,') ('Bush,', 'President') ('President', 'Obama,') ('Obama,', 'fellow') ('fellow', 'Americans') ('Americans', 'and') ('and', 'people') ('people', 'of') ('of', 'the') ('the', 'world,') ('world,', 'thank') ('thank', 'you.') ('you.', 'We,') ('We,', 'the') ('the', 'citizens') ('citizens', 'of') ('of', 'America') ('America', 'are') ('are', 'now') ('now', 'joined') ('joined', 'in') ('in', 'a') ('a', 'great') ('great', 'national') ('national', 'effort') ('effort', 'to') ('to', 'rebuild') ('rebuild', 'our') ('our', 'country') ('country', 'and') ('and', 'restore') ('restore', 'its') ('its', 'promise') ('promise', 'for') ('for', 'all') ('all', 'of') ('of', 'our') ('our', 'people.') ('people.', 'Together') ('Together', ',') (',', 'we') ('we', 'will') ('will', 'determine') ('determine', 'the') ('the', 'course') ('course', 'of') ('of', 'America') ('America', 'and') ('and', 'the') ('the', 'world') ('world', 'for') ('for', 'many,') ('many,', 'many') ('many', 'years') ('years', 'to') ('to', 'come.') ('come.', 'we') ('we', 'will') ('will', 'challenges.') ('challenges.', 'We') ('We', 'will') ('will', 'confront') ('confront', 'hardships,') ('hardships,', 'but') ('but', 'we') ('we', 'will') ('will', 'get') ('get', 'the') ('the', 'job') ('job', 'done.') "
     ]
    }
   ],
   "source": [
    "grams = ngrams(sentence.split(), 2)\n",
    "for gram in grams:\n",
    "    print(gram, end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Chief', 'Justice', 'Roberts,') ('Justice', 'Roberts,', 'President') ('Roberts,', 'President', 'Carter,') ('President', 'Carter,', 'President') ('Carter,', 'President', 'Clinton,') ('President', 'Clinton,', 'President') ('Clinton,', 'President', 'Bush,') ('President', 'Bush,', 'President') ('Bush,', 'President', 'Obama,') ('President', 'Obama,', 'fellow') ('Obama,', 'fellow', 'Americans') ('fellow', 'Americans', 'and') ('Americans', 'and', 'people') ('and', 'people', 'of') ('people', 'of', 'the') ('of', 'the', 'world,') ('the', 'world,', 'thank') ('world,', 'thank', 'you.') ('thank', 'you.', 'We,') ('you.', 'We,', 'the') ('We,', 'the', 'citizens') ('the', 'citizens', 'of') ('citizens', 'of', 'America') ('of', 'America', 'are') ('America', 'are', 'now') ('are', 'now', 'joined') ('now', 'joined', 'in') ('joined', 'in', 'a') ('in', 'a', 'great') ('a', 'great', 'national') ('great', 'national', 'effort') ('national', 'effort', 'to') ('effort', 'to', 'rebuild') ('to', 'rebuild', 'our') ('rebuild', 'our', 'country') ('our', 'country', 'and') ('country', 'and', 'restore') ('and', 'restore', 'its') ('restore', 'its', 'promise') ('its', 'promise', 'for') ('promise', 'for', 'all') ('for', 'all', 'of') ('all', 'of', 'our') ('of', 'our', 'people.') ('our', 'people.', 'Together') ('people.', 'Together', ',') ('Together', ',', 'we') (',', 'we', 'will') ('we', 'will', 'determine') ('will', 'determine', 'the') ('determine', 'the', 'course') ('the', 'course', 'of') ('course', 'of', 'America') ('of', 'America', 'and') ('America', 'and', 'the') ('and', 'the', 'world') ('the', 'world', 'for') ('world', 'for', 'many,') ('for', 'many,', 'many') ('many,', 'many', 'years') ('many', 'years', 'to') ('years', 'to', 'come.') ('to', 'come.', 'we') ('come.', 'we', 'will') ('we', 'will', 'challenges.') ('will', 'challenges.', 'We') ('challenges.', 'We', 'will') ('We', 'will', 'confront') ('will', 'confront', 'hardships,') ('confront', 'hardships,', 'but') ('hardships,', 'but', 'we') ('but', 'we', 'will') ('we', 'will', 'get') ('will', 'get', 'the') ('get', 'the', 'job') ('the', 'job', 'done.') "
     ]
    }
   ],
   "source": [
    "grams = ngrams(sentence.split(), 3)\n",
    "for gram in grams:\n",
    "    print(gram, end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 품사 Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 품사 분석 \n",
    "# KoNLPy - Kkma, Komoran, Hannanum, Twitter, Mecab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4차', '산업혁명', '이란', ',', '인공지능', ',', '사물인터넷', ',', '모바일,', '빅데이터', '등', '첨단', '정보통신', '기술', '이', '경제', '사회', '전반', '에', '융합', '되', '어', '혁신적', '이', 'ㄴ', '변화', '가', '나타나', '는', '차세대', '산업혁명', '을', '말하는데요~특히', '빅데이터', '는', '인공지능', 'AI', ',', '사물인터넷', ',', '5G', '등', '4차', '산업혁명', '의', '기술들', '의', '가장', '핵심', '이', '되', '는', '기술', '이', '라', '고', '하', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.morphs(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4차', '산업혁명', '인공지능', '사물인터넷', '모바일,', '빅데이터', '등', '첨단', '정보통신', '기술', '경제', '사회', '전반', '융합', '혁신적', '변화', '차세대', '산업혁명', '말하는데요~특히', '빅데이터', '인공지능', '사물인터넷', '5G', '등', '4차', '산업혁명', '기술들', '핵심', '기술', '수']\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.nouns(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4차', 'N'), ('산업혁명', 'N'), ('이란', 'J'), (',', 'S'), ('인공지능', 'N'), (',', 'S'), ('사물인터넷', 'N'), (',', 'S'), ('모바일,', 'N'), ('빅데이터', 'N'), ('등', 'N'), ('첨단', 'N'), ('정보통신', 'N'), ('기술', 'N'), ('이', 'J'), ('경제', 'N'), ('사회', 'N'), ('전반', 'N'), ('에', 'J'), ('융합', 'N'), ('되', 'X'), ('어', 'E'), ('혁신적', 'N'), ('이', 'J'), ('ㄴ', 'E'), ('변화', 'N'), ('가', 'J'), ('나타나', 'P'), ('는', 'E'), ('차세대', 'N'), ('산업혁명', 'N'), ('을', 'J'), ('말하는데요~특히', 'N'), ('빅데이터', 'N'), ('는', 'J'), ('인공지능', 'N'), ('AI', 'F'), (',', 'S'), ('사물인터넷', 'N'), (',', 'S'), ('5G', 'N'), ('등', 'N'), ('4차', 'N'), ('산업혁명', 'N'), ('의', 'J'), ('기술들', 'N'), ('의', 'J'), ('가장', 'M'), ('핵심', 'N'), ('이', 'J'), ('되', 'P'), ('는', 'E'), ('기술', 'N'), ('이', 'J'), ('라', 'E'), ('고', 'J'), ('하', 'P'), ('ㄹ', 'E'), ('수', 'N'), ('있', 'P'), ('습니다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "print(hannanum.pos(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "Kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '차', '산업', '혁명', '이란', ',', '인공지능', ',', '사물', '인터넷', ',', '모바일', ',', '빅', '데이터', '등', '첨단', '정보', '통신', '기술', '이', '경제', '사회', '전반', '에', '융합', '되', '어', '혁신적', '이', 'ㄴ', '변화', '가', '나타나', '는', '차세대', '산업', '혁명', '을', '말하', '는데요', '~', '특히', '빅', '데이터', '는', '인공지능', 'AI', ',', '사물', '인터넷', ',', '5', 'G', '등', '4', '차', '산업', '혁명', '의', '기술', '들', '의', '가장', '핵심', '이', '되', '는', '기술', '이', '라고', '하', 'ㄹ', '수', '있', '습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma.morphs(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '4차', '차', '산업', '산업혁명', '혁명', '인공', '인공지능', '지능', '사물', '사물인터넷', '인터넷', '모바일', '빅', '빅데이터', '데이터', '등', '첨단', '정보', '정보통신', '통신', '기술', '경제', '사회', '전반', '융합', '혁신적', '변화', '차세대', '5', '핵심', '수']\n"
     ]
    }
   ],
   "source": [
    "print(Kkma.nouns(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4', 'NR'), ('차', 'NNM'), ('산업', 'NNG'), ('혁명', 'NNG'), ('이란', 'JX'), (',', 'SP'), ('인공지능', 'NNG'), (',', 'SP'), ('사물', 'NNG'), ('인터넷', 'NNG'), (',', 'SP'), ('모바일', 'NNG'), (',', 'SP'), ('빅', 'NNG'), ('데이터', 'NNG'), ('등', 'NNB'), ('첨단', 'NNG'), ('정보', 'NNG'), ('통신', 'NNG'), ('기술', 'NNG'), ('이', 'JKS'), ('경제', 'NNG'), ('사회', 'NNG'), ('전반', 'NNG'), ('에', 'JKM'), ('융합', 'NNG'), ('되', 'XSV'), ('어', 'ECD'), ('혁신적', 'NNG'), ('이', 'VCP'), ('ㄴ', 'ETD'), ('변화', 'NNG'), ('가', 'JKS'), ('나타나', 'VV'), ('는', 'ETD'), ('차세대', 'NNG'), ('산업', 'NNG'), ('혁명', 'NNG'), ('을', 'JKO'), ('말하', 'VV'), ('는데요', 'ECD'), ('~', 'SO'), ('특히', 'MAG'), ('빅', 'NNG'), ('데이터', 'NNG'), ('는', 'JX'), ('인공지능', 'NNG'), ('AI', 'OL'), (',', 'SP'), ('사물', 'NNG'), ('인터넷', 'NNG'), (',', 'SP'), ('5', 'NR'), ('G', 'OL'), ('등', 'NNB'), ('4', 'NR'), ('차', 'NNM'), ('산업', 'NNG'), ('혁명', 'NNG'), ('의', 'JKG'), ('기술', 'NNG'), ('들', 'XSN'), ('의', 'JKG'), ('가장', 'MAG'), ('핵심', 'NNG'), ('이', 'JKC'), ('되', 'VV'), ('는', 'ETD'), ('기술', 'NNG'), ('이', 'VCP'), ('라고', 'ECD'), ('하', 'VV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('습니다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "print(Kkma.pos(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '차', '산업혁명', '이란', ',', '인공', '지능', ',', '사물인터넷', ',', '모바일', ',', '빅데이터', '등', '첨단', '정보', '통신', '기술', '이', '경제', '사회', '전반', '에', '융합', '되어', '혁신', '적', '인', '변화', '가', '나타나는', '차세대', '산업혁명', '을', '말', '하는데요', '~', '특히', '빅데이터', '는', '인공', '지능', 'AI', ',', '사물인터넷', ',', '5', 'G', '등', '4', '차', '산업혁명', '의', '기술', '들', '의', '가장', '핵심', '이', '되는', '기술', '이라고', '할', '수', '있습니다', '.']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.morphs(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['차', '산업혁명', '인공', '지능', '사물인터넷', '모바일', '빅데이터', '등', '첨단', '정보', '통신', '기술', '경제', '사회', '전반', '융합', '혁신', '변화', '차세대', '산업혁명', '말', '빅데이터', '인공', '지능', '사물인터넷', '등', '차', '산업혁명', '기술', '가장', '핵심', '기술', '수']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.nouns(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4', 'Number'), ('차', 'Noun'), ('산업혁명', 'Noun'), ('이란', 'Josa'), (',', 'Punctuation'), ('인공', 'Noun'), ('지능', 'Noun'), (',', 'Punctuation'), ('사물인터넷', 'Noun'), (',', 'Punctuation'), ('모바일', 'Noun'), (',', 'Punctuation'), ('빅데이터', 'Noun'), ('등', 'Noun'), ('첨단', 'Noun'), ('정보', 'Noun'), ('통신', 'Noun'), ('기술', 'Noun'), ('이', 'Josa'), ('경제', 'Noun'), ('사회', 'Noun'), ('전반', 'Noun'), ('에', 'Josa'), ('융합', 'Noun'), ('되어', 'Verb'), ('혁신', 'Noun'), ('적', 'Suffix'), ('인', 'Josa'), ('변화', 'Noun'), ('가', 'Josa'), ('나타나는', 'Verb'), ('차세대', 'Noun'), ('산업혁명', 'Noun'), ('을', 'Josa'), ('말', 'Noun'), ('하는데요', 'Verb'), ('~', 'Punctuation'), ('특히', 'Adverb'), ('빅데이터', 'Noun'), ('는', 'Josa'), ('인공', 'Noun'), ('지능', 'Noun'), ('AI', 'Alpha'), (',', 'Punctuation'), ('사물인터넷', 'Noun'), (',', 'Punctuation'), ('5', 'Number'), ('G', 'Alpha'), ('등', 'Noun'), ('4', 'Number'), ('차', 'Noun'), ('산업혁명', 'Noun'), ('의', 'Josa'), ('기술', 'Noun'), ('들', 'Suffix'), ('의', 'Josa'), ('가장', 'Noun'), ('핵심', 'Noun'), ('이', 'Josa'), ('되는', 'Verb'), ('기술', 'Noun'), ('이라고', 'Josa'), ('할', 'Verb'), ('수', 'Noun'), ('있습니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter.pos(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4차', '4차 산업혁명', '인공지능', '사물인터넷', '모바일', '빅데이터', '빅데이터 등', '빅데이터 등 첨단', '빅데이터 등 첨단 정보통신', '빅데이터 등 첨단 정보통신 \\n                      기술', '경제', '경제 사회', '경제 사회 전반', '융합', '변화', '차세대', '차세대 산업혁명', '인공지능 AI', '5G', '5G 등', '5G 등 4차', '5G 등 4차 산업혁명', '5G 등 4차 산업혁명의 \\n                      기술들', '5G 등 4차 산업혁명의 \\n                      기술들의 가장', '기술', '산업혁명', '인공', '지능', '첨단', '정보', '통신', '사회', '전반', 'AI', '기술들', '가장', '핵심']\n"
     ]
    }
   ],
   "source": [
    "print(twitter.phrases(\"\"\"4차 산업혁명이란, 인공지능, 사물인터넷, 모바일, 빅데이터 등 첨단 정보통신 \n",
    "                      기술이 경제 사회 전반에 융합되어 혁신적인 변화가 나타나는 차세대 산업혁명을  \n",
    "                      말하는데요~특히 빅데이터는 인공지능 AI, 사물인터넷, 5G 등 4차 산업혁명의 \n",
    "                      기술들의 가장 핵심이 되는 기술이라고 할 수 있습니다.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "sentence = \"\"\"In computer science, artificial intelligence (AI), sometimes called machine intelligence, \n",
    "is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans \n",
    "and animals. Colloquially, the term \"artificial intelligence\" is used to describe machines that mimic \n",
    "\"cognitive\" functions that humans associate with other human minds, such as \"learning\" and \"problem solving\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'computer', 'science,', 'artificial', 'intelligence', '(AI),', 'sometimes', 'called', 'machine', 'intelligence,', 'is', 'intelligence', 'demonstrated', 'by', 'machines,', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'displayed', 'by', 'humans', 'and', 'animals.', 'Colloquially,', 'the', 'term', '\"artificial', 'intelligence\"', 'is', 'used', 'to', 'describe', 'machines', 'that', 'mimic', '\"cognitive\"', 'functions', 'that', 'humans', 'associate', 'with', 'other', 'human', 'minds,', 'such', 'as', '\"learning\"', 'and', '\"problem', 'solving\".']\n"
     ]
    }
   ],
   "source": [
    "tokens = sentence.split()\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN'), ('computer', 'NN'), ('science,', 'VBP'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('(AI),', 'NN'), ('sometimes', 'RB'), ('called', 'VBD'), ('machine', 'NN'), ('intelligence,', 'NN'), ('is', 'VBZ'), ('intelligence', 'RB'), ('demonstrated', 'VBN'), ('by', 'IN'), ('machines,', 'NN'), ('in', 'IN'), ('contrast', 'NN'), ('to', 'TO'), ('the', 'DT'), ('natural', 'JJ'), ('intelligence', 'NN'), ('displayed', 'VBN'), ('by', 'IN'), ('humans', 'NNS'), ('and', 'CC'), ('animals.', 'JJ'), ('Colloquially,', 'NNP'), ('the', 'DT'), ('term', 'NN'), ('\"artificial', 'JJ'), ('intelligence\"', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('describe', 'VB'), ('machines', 'NNS'), ('that', 'IN'), ('mimic', 'JJ'), ('\"cognitive\"', 'JJ'), ('functions', 'NNS'), ('that', 'WDT'), ('humans', 'NNS'), ('associate', 'VBP'), ('with', 'IN'), ('other', 'JJ'), ('human', 'JJ'), ('minds,', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('\"learning\"', 'NN'), ('and', 'CC'), ('\"problem', 'NNP'), ('solving\".', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tags = pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chief Justice Roberts, President Carter, President Clinton, President Bush,',\n",
       " 'President Obama, fellow Americans and people of the world,',\n",
       " 'thank you. We, the citizens of America are now joined in a',\n",
       " 'great national effort to rebuild our country and restore its promise',\n",
       " 'for all of our people. Together , we will determine the course of America',\n",
       " 'and the world for many, many years to come. we will',\n",
       " 'challenges. We will confront hardships, but we will get the job done.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['Chief Justice Roberts, President Carter, President Clinton, President Bush,', \n",
    "'President Obama, fellow Americans and people of the world,', \n",
    "'thank you. We, the citizens of America are now joined in a',\n",
    "  'great national effort to rebuild our country and restore its promise', \n",
    "    'for all of our people. Together , we will determine the course of America', \n",
    "    'and the world for many, many years to come. we will', \n",
    "  'challenges. We will confront hardships, but we will get the job done.']\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=20000, min_df=2,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word', \n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None, \n",
    "                             stop_words = None, \n",
    "                             min_df = 2, # 토큰이 나타날 최소 문서 개수\n",
    "                             ngram_range=(1, 3),  # 단어를 몇 개씩 묶어서\n",
    "                             max_features = 20000)  # 가방에 토큰을 최대 몇 개까지)\n",
    "                             \n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x15 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 39 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence = vectorizer.fit_transform(sentence)\n",
    "clean_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['america',\n",
       " 'and',\n",
       " 'for',\n",
       " 'of',\n",
       " 'of america',\n",
       " 'our',\n",
       " 'people',\n",
       " 'president',\n",
       " 'the',\n",
       " 'the world']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>america</th>\n",
       "      <th>and</th>\n",
       "      <th>for</th>\n",
       "      <th>of</th>\n",
       "      <th>of america</th>\n",
       "      <th>our</th>\n",
       "      <th>people</th>\n",
       "      <th>president</th>\n",
       "      <th>the</th>\n",
       "      <th>the world</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>we will</th>\n",
       "      <th>will</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   america  and  for  of  of america  our  people  president  the  the world  \\\n",
       "0        0    0    0   0           0    0       0          3    0          0   \n",
       "1        0    1    0   1           0    0       1          1    1          1   \n",
       "2        1    0    0   1           1    0       0          0    1          0   \n",
       "3        0    1    0   0           0    1       0          0    0          0   \n",
       "4        1    0    1   2           1    1       1          0    1          0   \n",
       "5        0    1    1   0           0    0       0          0    1          1   \n",
       "6        0    0    0   0           0    0       0          0    1          0   \n",
       "\n",
       "   to  we  we will  will  world  \n",
       "0   0   0        0     0      0  \n",
       "1   0   0        0     0      1  \n",
       "2   0   1        0     0      0  \n",
       "3   1   0        0     0      0  \n",
       "4   0   1        1     1      0  \n",
       "5   1   1        1     1      1  \n",
       "6   0   2        2     2      0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(clean_sentence.toarray(), columns=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
